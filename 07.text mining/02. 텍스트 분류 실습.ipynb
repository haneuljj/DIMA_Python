{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c3ea8bb-04e4-4433-9a3e-f2dd3b30f7f7",
   "metadata": {},
   "source": [
    "# 20 뉴스그룹 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b94352-fdea-4eeb-89ff-e224309cca53",
   "metadata": {},
   "source": [
    "## 텍스트 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b20bfae3-bba4-44d1-b57d-8111be9a4c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "news_data = fetch_20newsgroups(subset='all', random_state=100)  # subset=all: 훈련, 평가데이터 모두 섞어 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "977b0c2e-316e-434c-98a1-96daa8cb8ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils._bunch.Bunch'>\n",
      "(18846,)\n"
     ]
    }
   ],
   "source": [
    "print(type(news_data))\n",
    "print(np.array(news_data.data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "485e2409-b5ec-4bb2-9965-c2641972a7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 클래스\n",
      "{'alt.atheism': 799, 'comp.graphics': 973, 'comp.os.ms-windows.misc': 985, 'comp.sys.ibm.pc.hardware': 982, 'comp.sys.mac.hardware': 963, 'comp.windows.x': 988, 'misc.forsale': 975, 'rec.autos': 990, 'rec.motorcycles': 996, 'rec.sport.baseball': 994, 'rec.sport.hockey': 999, 'sci.crypt': 991, 'sci.electronics': 984, 'sci.med': 990, 'sci.space': 987, 'soc.religion.christian': 997, 'talk.politics.guns': 910, 'talk.politics.mideast': 940, 'talk.politics.misc': 775, 'talk.religion.misc': 628}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pandasd\n",
    "\n",
    "print('target 클래스')\n",
    "print(dict(zip(np.unique(news_data.target_names), np.bincount(news_data.target))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7665a94-f9a3-4f7d-af9b-36cee1e3153a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: ggr@koonda.acci.com.au (Greg Rose)\n",
      "Subject: Authentication and one-time-pads (was: Re: Advanced one time pad)\n",
      "Summary: presents one-time-pad based MAC\n",
      "Organization: Australian Computing and Communications Institute\n",
      "Lines: 93\n",
      "\n",
      "In article <1s1dbmINNehb@elang05.acslab.umbc.edu> olson@umbc.edu (Bryan Olson; CMSC (G)) writes:\n",
      ">The one-time-pad yeilds ideal security, but has a well-known flaw in\n",
      ">authentication.  Suppose you use a random bit stream as the pad, and\n",
      ">exclusive-or as the encryption operation.  If an adversary knows the \n",
      ">plaintext of a message, he can change it into any other message.  \n",
      ">Here's how it works.\n",
      ">\n",
      ">Alice is sending Bob a plaintext P, under a key stream S\n",
      ">Alice computes the ciphertext C = S xor P,  and sends it to Bob.\n",
      ">\n",
      ">Eve knows the plainext P, but wants the message to appear as P'.\n",
      ">Eve intercepts C, and computes  C' = C xor P xor P' = S xor P'.\n",
      ">Eve sends C' to Bob.\n",
      ">\n",
      ">Bob decrypts C' by computing  C'xor S = P',  thus receiving the \n",
      ">false message which was substituted by Eve.\n",
      "\n",
      "Firstly, an aside:\n",
      "\n",
      "I agree that the weakness exists, but I have a lot of trouble\n",
      "believing that it represents a difficulty in real life. Given:\n",
      "\n",
      "1. the purpose of the one-time pad is to give unbreakable security,\n",
      "and the expense of key distribution etc., imply that the clients\n",
      "really do want that level of security\n",
      "\n",
      "2. These same people want to keep P a secret\n",
      "\n",
      "I find it hard to believe that Eve might happen to have a copy of P\n",
      "lying around.\n",
      "\n",
      "(I am aware that the same argument applies to Eve knowing even a small\n",
      "part of the message, but Eve must know EXACTLY where (which bytes) in\n",
      "C her known susequence starts, or the result will be garbled. I find\n",
      "this at least as surprising.)\n",
      "\n",
      "Back to the question:\n",
      "\n",
      "If I had the resources to use a one-time-pad for such transmissions, I\n",
      "would also append a Message Authentication Code to the message, using up\n",
      "the next bits of the one-time-pad as the key perhaps. Your original\n",
      "question basically asked whether there was any way to authenticate the\n",
      "message with the same degree of security as the Pad itself provided,\n",
      "and I don't know the answer. However, I would propose the following\n",
      "for discussion.\n",
      "\n",
      "Alice and Bob have an arbitrary number of secret, random bits to\n",
      "share, which Eve doesn't know. She finds them out (effectively) by\n",
      "knowing some P and the corresponding C. It is the fact that they\n",
      "CORRESPOND that causes the problem. If a message authentication code was to\n",
      "be created using some one-time-pad operation such that Eve could not\n",
      "know which parts of the MAC were affected by which parts of the input,\n",
      "she would be unable to forge a MAC to correspond.\n",
      "\n",
      "What is required is a non-linear combiner of parts of the message.\n",
      "(Non-linear so that simply xoring or subtracting or whatever doesn't\n",
      "have exactly the same effect).\n",
      "\n",
      "Now, at the end of the encrypted message C, Alice appends a n-bit MAC\n",
      "computed as follows (S2 means the next full chunk of the one time pad):\n",
      "  1. compute C2 = P xor S2, and pad to an n-bit boundary with more of S\n",
      "  2. break C2 into n-bit chunks\n",
      "  3. set MAC to 0 (initialisation vector)\n",
      "  4. for i in each chunk sequentially\n",
      "       set MAC = MAC NLOP C2[i]\n",
      "\n",
      "At the end of this process MAC is the Message Authentication Code.\n",
      "\n",
      "(Bob verifies the MAC in the obvious manner; he recovers the\n",
      "plaintext P, then uses some more of his pad to reproduce the MAC in\n",
      "the same manner.)\n",
      "\n",
      "NLOP is the non-linear operator, and there is the rub. The simplest\n",
      "non-linear operator I can think of is an S-box; that is, have a fixed\n",
      "(even published) permutation of the n-bit integers, an indexable table\n",
      "called Sbox, and use:\n",
      "  x NLOP y = x xor Sbox[y].\n",
      "\n",
      "Practically speaking, I think this solves your problem, as Eve never\n",
      "sees the intermediate output C2, and hence can't deduce S2 or perform any valid\n",
      "substitution on it.\n",
      "\n",
      "Also practically speaking, you want the MAC to be fairly large, say 32\n",
      "bits, but you might not want a 4 gigabyte (say) S-box, so you might\n",
      "work on 4 byte-sized S-boxes, but I think that is an irrelevant detail\n",
      "for the discussion at hand.\n",
      "\n",
      "Who will be first to point out my errors, or give me a pointer to some\n",
      "literature?\n",
      "--\n",
      "Greg Rose                 Australian Computing and Communications Institute\n",
      "ggr@acci.com.au                                              +61 18 174 842\n",
      "`Use of the standard phrase \"HIJACKED\" may be inadvisable' -- CAA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0954ea4-5086-45bb-a617-00922f1fcfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b87c82f-6396-4a4e-bf56-304e073efa07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Firstly, an aside:\n",
      "\n",
      "I agree that the weakness exists, but I have a lot of trouble\n",
      "believing that it represents a difficulty in real life. Given:\n",
      "\n",
      "1. the purpose of the one-time pad is to give unbreakable security,\n",
      "and the expense of key distribution etc., imply that the clients\n",
      "really do want that level of security\n",
      "\n",
      "2. These same people want to keep P a secret\n",
      "\n",
      "I find it hard to believe that Eve might happen to have a copy of P\n",
      "lying around.\n",
      "\n",
      "(I am aware that the same argument applies to Eve knowing even a small\n",
      "part of the message, but Eve must know EXACTLY where (which bytes) in\n",
      "C her known susequence starts, or the result will be garbled. I find\n",
      "this at least as surprising.)\n",
      "\n",
      "Back to the question:\n",
      "\n",
      "If I had the resources to use a one-time-pad for such transmissions, I\n",
      "would also append a Message Authentication Code to the message, using up\n",
      "the next bits of the one-time-pad as the key perhaps. Your original\n",
      "question basically asked whether there was any way to authenticate the\n",
      "message with the same degree of security as the Pad itself provided,\n",
      "and I don't know the answer. However, I would propose the following\n",
      "for discussion.\n",
      "\n",
      "Alice and Bob have an arbitrary number of secret, random bits to\n",
      "share, which Eve doesn't know. She finds them out (effectively) by\n",
      "knowing some P and the corresponding C. It is the fact that they\n",
      "CORRESPOND that causes the problem. If a message authentication code was to\n",
      "be created using some one-time-pad operation such that Eve could not\n",
      "know which parts of the MAC were affected by which parts of the input,\n",
      "she would be unable to forge a MAC to correspond.\n",
      "\n",
      "What is required is a non-linear combiner of parts of the message.\n",
      "(Non-linear so that simply xoring or subtracting or whatever doesn't\n",
      "have exactly the same effect).\n",
      "\n",
      "Now, at the end of the encrypted message C, Alice appends a n-bit MAC\n",
      "computed as follows (S2 means the next full chunk of the one time pad):\n",
      "  1. compute C2 = P xor S2, and pad to an n-bit boundary with more of S\n",
      "  2. break C2 into n-bit chunks\n",
      "  3. set MAC to 0 (initialisation vector)\n",
      "  4. for i in each chunk sequentially\n",
      "       set MAC = MAC NLOP C2[i]\n",
      "\n",
      "At the end of this process MAC is the Message Authentication Code.\n",
      "\n",
      "(Bob verifies the MAC in the obvious manner; he recovers the\n",
      "plaintext P, then uses some more of his pad to reproduce the MAC in\n",
      "the same manner.)\n",
      "\n",
      "NLOP is the non-linear operator, and there is the rub. The simplest\n",
      "non-linear operator I can think of is an S-box; that is, have a fixed\n",
      "(even published) permutation of the n-bit integers, an indexable table\n",
      "called Sbox, and use:\n",
      "  x NLOP y = x xor Sbox[y].\n",
      "\n",
      "Practically speaking, I think this solves your problem, as Eve never\n",
      "sees the intermediate output C2, and hence can't deduce S2 or perform any valid\n",
      "substitution on it.\n",
      "\n",
      "Also practically speaking, you want the MAC to be fairly large, say 32\n",
      "bits, but you might not want a 4 gigabyte (say) S-box, so you might\n",
      "work on 4 byte-sized S-boxes, but I think that is an irrelevant detail\n",
      "for the discussion at hand.\n",
      "\n",
      "Who will be first to point out my errors, or give me a pointer to some\n",
      "literature?\n"
     ]
    }
   ],
   "source": [
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c2572-7aef-49b4-8562-ff2e31d6dbab",
   "metadata": {},
   "source": [
    "## 학습 데이터/평가 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0629ea26-ccc9-4a3c-88c2-3ddda24e1ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터 크기: 11314, 평가데이터 크기:7532\n"
     ]
    }
   ],
   "source": [
    "train_news = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=100)\n",
    "x_train = train_news.data\n",
    "y_train = train_news.target\n",
    "\n",
    "test_news = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), random_state=100)\n",
    "x_test = test_news.data\n",
    "y_test = test_news.target\n",
    "\n",
    "print(f'학습데이터 크기: {len(x_train)}, 평가데이터 크기:{len(x_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b04c2d-c80e-4a09-9a0f-96d44ebfb33e",
   "metadata": {},
   "source": [
    "## 피처 벡터화 변환과 머신러닝 모델 학습/예측/평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c74ee9-fa03-441e-9c22-bb432b9e503c",
   "metadata": {},
   "source": [
    "### countvectorizer 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b33cf76-9ebb-4266-8a7c-321d2e29457d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터 텍스트 cnt vect의 shape: (11314, 101487)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "cnt_vect = CountVectorizer(stop_words = stopwords.words('english'))\n",
    "\n",
    "# fit, transform을 나눈 이유: 학습데이터에서 학습한 기준으로 평가데이터도 학습 필요\n",
    "cnt_vect.fit(x_train)\n",
    "x_train_cnt_vect = cnt_vect.transform(x_train)\n",
    "\n",
    "x_test_cnt_vect = cnt_vect.transform(x_test)\n",
    "print(f'학습데이터 텍스트 cnt vect의 shape: {x_train_cnt_vect.shape}') # 열: 학습데이터에 있는 토큰화된 단어의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb71c478-7c1f-4301-96a3-71facbb96b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.635\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr.fit(x_train_cnt_vect, y_train)  # 종속변수는 0~19의 숫자를 가짐(뉴스 20개의 카테고리)\n",
    "\n",
    "y_hat = lr.predict(x_test_cnt_vect)\n",
    "print(f'정확도: {accuracy_score(y_test, y_hat):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a82bee6-37aa-4aaa-aad7-ed16d1134ad0",
   "metadata": {},
   "source": [
    "### tfidfvectorizer 이용 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee4a2dc6-3182-49eb-81df-87f41f543a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터 텍스트 cnt vect의 shape: (11314, 101487)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(stop_words = stopwords.words('english'))\n",
    "\n",
    "tfidf_vect.fit(x_train)\n",
    "x_train_tfidf_vect = tfidf_vect.transform(x_train)\n",
    "\n",
    "x_test_tfidf_vect = tfidf_vect.transform(x_test)\n",
    "print(f'학습데이터 텍스트 cnt vect의 shape: {x_train_tfidf_vect.shape}')\n",
    "# 영문서 빈도: 특정단어가 다른 문서에도 반복적으로 나오면 패널티 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc790d7c-4006-4835-bdea-e05ee6325270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.688\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr.fit(x_train_tfidf_vect, y_train)  \n",
    "\n",
    "y_hat = lr.predict(x_test_tfidf_vect)\n",
    "print(f'정확도: {accuracy_score(y_test, y_hat):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c43776-02e0-4126-b521-1ba2d3c6ac98",
   "metadata": {},
   "source": [
    "## tfidfvectorizer 이용 - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683fdf44-3cfb-4c88-b89a-c9d53ac38947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_df : 전체 문서에 걸쳐서 너무 높은 빈도수를 가지는 단어 피처를 제외\n",
    "# 너무 높은 빈도수를 가지는 단어는 불용어오 같이 단어로서의 의미가 없는 반복적인 단어일 확률이 높다\n",
    "# max_df=300 : 전체 문서에 걸쳐 300개 이하로 나오는 단어만 피처로 추출\n",
    "tfidf_vec = TfidfVectorizer(stop_words = stopwords.words('english'), ngram_range=(1,2), max_df=300) \n",
    "tfidf_vec.fit(x_train)\n",
    "x_train_tfidf_vec = tfidf_vec.transform(x_train) # 왜 나눠서 하나? 테스트 데이터도 학습 데이터와 같은 기준으로 transform 해야되기 때문에\n",
    "\n",
    "x_test_tfidf_vec = tfidf_vec.transform(x_test)\n",
    "print(f'학습 데이터 텍스트 tfidf vect shape: {x_train_tfidf_vec.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7955c00-0d30-408d-9611-91467c1cd815",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr.fit(x_train_tfidf_vec, y_train)\n",
    "\n",
    "y_hat = lr.predict(x_test_tfidf_vec)\n",
    "print(f'정확도: {accuracy_score(y_test, y_hat):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
