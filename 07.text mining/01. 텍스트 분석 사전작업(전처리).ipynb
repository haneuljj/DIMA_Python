{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35acab1b-ed5f-4965-b8f2-a1bcacde91c2",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56241e2-7163-4d88-9141-738807d84ec5",
   "metadata": {},
   "source": [
    "## 문장 토큰화\n",
    "- https://www.nltk.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6d1ef86-c597-42d3-8361-da6979186a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b1d2aa4-5e9b-4b61-97b9-917549cc84d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a68a0d-edbd-4319-9fbd-5e38f7a2a81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')  # 불용어 사전 다운로드\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "sentences = sent_tokenize(text_sample)  # 문서를 문장 단위로 분리\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26952b9b-a906-436f-9e88-0a56b42237c0",
   "metadata": {},
   "source": [
    "## 단어 토큰화\n",
    "- 문장을 단어로 토큰화하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc8bb48a-ff7e-4e04-83af-f500307cca04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentence)  # 공백 기준으로 문장을 단어로 분리, 불용어도 같이 분리됨\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1093085-dbeb-4193-933f-fd4a6d7de054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bcf7084-e9c0-438b-8b69-bfdb04adb8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "word_tokens = tokenize_text(text_sample)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281ab8fa-3cc5-415d-8469-7997d97a846f",
   "metadata": {},
   "source": [
    "## stopwords 제거\n",
    "- stopwords: 텍스트 분석에 의미가 없는 단어를 지칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad7bae6e-bd55-4450-a5ce-fa534bfc809a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')  # 불용어 목록을 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aeccc03-3e40-4763-bd21-43fd4c9b6277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 목록 지원언어\n",
    "nltk.corpus.stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72cf32be-0a04-4e6d-b956-28bdf0c5f98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stopwords 개수: 179\n"
     ]
    }
   ],
   "source": [
    "print('영어 stopwords 개수:', len(nltk.corpus.stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dffa0b17-8398-42f8-b735-b39a1e98a3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73a99c3a-7522-499e-be01-31b241179b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[원본단어]\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n",
      "[불용어 제거 단어]\n",
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "print('[원본단어]')\n",
    "print(word_tokens)\n",
    "\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []   # 불용어 제거한 단어 저장할 리스트\n",
    "    for word in sentence:\n",
    "        word = word.lower()  # 소문자로 변환\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print('[불용어 제거 단어]')\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2644f362-75b7-4261-a916-ad23545e2dde",
   "metadata": {},
   "source": [
    "## 어간추출(Stemming)과 표제어 추출(Lemmatization)\n",
    "- 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는다\n",
    "- ex) worked, works, working의 동사원형인 work를 찾아준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec98db61-0e5f-468a-957e-8553053e7ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "happy happiest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))  # 동사 원형 찾아줌\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))   #  정확하게 찾지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a54b9083-eabf-4538-8246-6b66bada1f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy happy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('happier', 'a') , lemma.lemmatize('happiest', 'a')) # 매개변수로 품사입력, 명사(n), 동사(v), 형용사(a), 부사(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e25175-cc84-4d42-af74-4ff4295b10aa",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be20e30-a751-4d43-9d12-41820dda5744",
   "metadata": {},
   "source": [
    "## DTM(Document Term Matrix, 문서 단어 행렬)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a51f56f-d636-4d64-97f0-95660fa25762",
   "metadata": {},
   "source": [
    "- CountVectorizer 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0809ab20-2698-464d-9223-936720041de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Vector:   (0, 4)\t2\n",
      "  (0, 1)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 2)\t2\n",
      "  (0, 0)\t1\n",
      "BoW Vector: [[1 1 2 1 2 1]]\n",
      "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love. because I Love you']\n",
    "cv = CountVectorizer()\n",
    "\n",
    "print('BoW Vector:', cv.fit_transform(corpus))  # 밀집벡터 형태로\n",
    "print('BoW Vector:', cv.fit_transform(corpus).toarray())\n",
    "# 'I'가 BoW를 만드는 과정에서 사라진 이유는 countvectorizer가 기본적으로 길이가 2이상인 문자에 대해서만 토큰으로 인식하기 때문에 포함이 안됨\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c32dee5-29c3-4a24-8901-dc01add1e0af",
   "metadata": {},
   "source": [
    "- 불용어를 제거한 BoW 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792fd3be-e693-4839-95e1-0bf353c46803",
   "metadata": {},
   "source": [
    "(1) 사용자 정의 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91201d70-a0bf-4929-b290-a9476d6956c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Vector: [[1 1 1 1 1 1]]\n",
      "{'family': 1, 'not': 4, 'important': 2, 'thing': 5, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "cv = CountVectorizer(stop_words=['the', 'a', 'an', 'is'])\n",
    "\n",
    "print('BoW Vector:', cv.fit_transform(text).toarray())\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45df170a-9d66-4edc-8aab-fd30a7027259",
   "metadata": {},
   "source": [
    "(2) nltk에서 지원하는 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5b125ff-3233-4908-b131-a1c5794bba33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Vector: [[1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "stop_words = stopwords.words('english')\n",
    "cv = CountVectorizer(stop_words = stop_words)\n",
    "\n",
    "print('BoW Vector:', cv.fit_transform(text).toarray())\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3959335-21e3-42b0-9f2b-e72a14b687a0",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "- TfidfVectorizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb8f63ba-fd85-4dd0-8f05-60bb42d15edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['you know I want your love', # 문서1\n",
    "          'I Like you', # 문서2\n",
    "          'what should I do']  # 문서3\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "print(tfidf.fit_transform(corpus).toarray())  # 문서별로 빈도 ? \n",
    "print(tfidf.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
