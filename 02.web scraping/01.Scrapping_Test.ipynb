{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa99d8f7-6d4d-400a-95d4-42ddddc2c51c",
   "metadata": {},
   "source": [
    "# request 패키지\n",
    "\n",
    "## 요청의 종류 \n",
    "1. GET ( 정보를 가져오기 위해 요청 )\n",
    "2. POST ( 새로운 정보를 보내기 위해 요청 )\n",
    "3. PUT ( 수정할 정보를 보내기 위해 요청 )\n",
    "4. DELETE ( 정보를 삭제하기 위해 요청 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa17713b-ae9f-4070-bbfd-444351e92a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be9ce367-86a1-4956-a342-3552f5303423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "reprsponse = requests.get(\"https://www.naver.com\")  # get 방식으로 요청\n",
    "print(response.status_code)   # 200 : 성공적으로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b4e3c5-3ae3-477a-8d16-0b87c9f86500",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(response.text)   # 네이버의 홈페이지에 대한 html 텍스트 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dac2a4-e855-4634-8eee-23c1d2600076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "URL = \"https://search.naver.com/search.naver\"\n",
    "query = {'query':'python'} \n",
    "\n",
    "response = requests.get(URL, params=query)  # 해당 url의 파라미터 문자열 정보 입력, query string으로 보내짐\n",
    "print(response.status_code)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1652ba99-1466-40a0-8426-0a2e77cc899f",
   "metadata": {},
   "source": [
    "## 로봇이 아님을 나타내기 위해 user-agent라는 값을 헤더에 넣어서 보내기\n",
    "- 직접적인 URL 주소로 요청시 웹 사이트에서 웹 크롤링을 통해서 접근한 것을 감지하고 접속을 차단하게 되는 경우 존재\n",
    "- user-agent 헤더 값을 포함하여 요청하면 웹 브라우저를 통해 요청하는 것으로 인식하게 되어 문제 해결 가능\n",
    "- user agent값 확인하는 법: 웹 브라우저 실행 -> F12를 눌러 개발자 모드 -> Console -> 프롬프트에 'navigator.userAgent'입력\n",
    "- \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b91fe3e5-c4ac-4c0d-a777-5d446331f0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장완료\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "URL = \"http://www.google.com/search\"\n",
    "params = {'q':'python'}\n",
    "\n",
    "# 로봇이 아님을 나타내기 위해 user-agent라는 값을 header에 넣어서 전송\n",
    "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36'}\n",
    "\n",
    "# user-agent 값을 통해 브라우저의 종류와 버전 정보 등을 전달하기 때문에 웹 프로그램은 브라우저별로 최적화된 콘텐츠를 제공\n",
    "# header 정보를 전달했을때와 안했을 때의 차이\n",
    "response = requests.get(URL, params=params)#, headers=headers)\n",
    "response.raise_for_status()    # 200이 아니면 오류를 내고 중지, 밑 문장 실행x\n",
    "\n",
    "\n",
    "html = response.text\n",
    "with open('search_result2.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(html)\n",
    "print('저장완료')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c4fba-04f7-4591-84b1-51dcecdcdf64",
   "metadata": {},
   "source": [
    "[실습] 네이버 실시간 인기 검색어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eee0977-c3c4-48ed-b6a3-49c6365d9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://datalab.naver.com')\n",
    "html = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80ed8661-2082-4c72-80dd-402ae7be22b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경량패딩\n"
     ]
    }
   ],
   "source": [
    "temp = html.split('<em class=\"num\">1</em>')[1]\n",
    "temp = temp.split('<span class=\"title\">')[1]\n",
    "temp = temp.split('</span>')[0]\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ac55b0-ae51-4a79-acca-7dff7dcd3eca",
   "metadata": {},
   "source": [
    "# BeautifulSoup 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b13835a-e041-457b-8fe0-58434e79a684",
   "metadata": {},
   "source": [
    "## parser별 출력 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed25d10f-2847-458b-9a66-d8685cf5af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c020252-a68f-47bc-9c7f-bcacc0070618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html.parser\n",
      "<a></a>\n",
      "----------------------------------------\n",
      "lxml\n",
      "<html><body><a></a></body></html>\n",
      "----------------------------------------\n",
      "xml\n",
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "<a/>\n",
      "----------------------------------------\n",
      "html5lib\n",
      "<html><head></head><body><a><p></p></a></body></html>\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup =  BeautifulSoup('<a></p>', 'html.parser')\n",
    "print('html.parser')\n",
    "print(soup)\n",
    "print('-'*40)\n",
    "\n",
    "soup =  BeautifulSoup('<a></p>', 'lxml')\n",
    "print('lxml')\n",
    "print(soup)\n",
    "print('-'*40)\n",
    "\n",
    "soup =  BeautifulSoup('<a></p>', 'xml')   # xml문서를 파싱하는데 특화\n",
    "print('xml')\n",
    "print(soup)\n",
    "print('-'*40)\n",
    "\n",
    "soup =  BeautifulSoup('<a></p>', 'html5lib')\n",
    "print('html5lib')\n",
    "print(soup)\n",
    "print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b88c8e-f62d-43c1-9821-6ef0bfe5ce39",
   "metadata": {},
   "source": [
    "## 기본 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea84d5dc-8eb1-40cb-b905-1ccdd065fb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "웹 크롤러 - 위키백과, 우리 모두의 백과사전\n",
      " 이 문서는 2023년 4월 30일 (일) 18:34에 마지막으로 편집되었습니다.\n",
      "모든 문서는 크리에이티브 커먼즈 저작자표시-동일조건변경허락 4.0에 따라 사용할 수 있으며, 추가적인 조건이 적용될 수 있습니다. 자세한 내용은 이용 약관을 참고하십시오.Wikipedia®는 미국 및 다른 국가에 등록되어 있는 Wikimedia Foundation, Inc. 소유의 등록 상표입니다.\n",
      "<a class=\"mw-jump-link\" href=\"#bodyContent\">본문으로 이동</a>\n",
      "#bodyContent\n",
      "<a href=\"/wiki/%EA%B5%AC%EA%B8%80%EB%B4%87\" title=\"구글봇\">구글봇</a>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://ko.wikipedia.org/wiki/%EC%9B%B9_%ED%81%AC%EB%A1%A4%EB%9F%AC'\n",
    "res = requests.get(url)\n",
    "\n",
    "# soup 객체 생성\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "# 태그를 이용한 접근\n",
    "print(soup.title.text)   # title 태그 접근, .text: 태그를 제외한 컨텐츠만 출력, .name: 태그의 이름만 출력\n",
    "print(soup.footer.ul.li.text)\n",
    "print(soup.footer.ul.li.find_next_sibling().text)\n",
    "\n",
    "# 태그와 속성을 이용한 접근\n",
    "print(soup.a) # a 태그 접근, soup 객체에서 첫번째로 만나는 a element 를 출력\n",
    "print(soup.a['href'])  # a 태그의 href 속성값 출력\n",
    "# print(soup.a['id'])  # 속성이 존재하지 않을 경우 에러 발생\n",
    "\n",
    "# find()함수를 이용한 태그 내의 다양한 속성을 이용한 접근\n",
    "print(soup.find('a', attrs={'title':'구글봇'}))   # a 태그의 title 속성 중 값이 구글봇인 것 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6ccfb-5e1d-47a2-85f3-cdca0f44143c",
   "metadata": {},
   "source": [
    "## 자식 노드(태그)들을 반복 가능한 객체로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25b24757-be7c-4354-8d50-3072f5dacad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<p align=\"center\" class=\"a\">text1</p>\n",
      "\n",
      "\n",
      "<p align=\"center\" class=\"b\">text2</p>\n",
      "\n",
      "\n",
      "<p align=\"center\" class=\"c\">text3</p>\n",
      "\n",
      "\n",
      "<div>\n",
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "</div>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html = \"\"\"\n",
    "<html>\n",
    "    <head>\n",
    "        <title>crawl</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class='a' align='center'>text1</p>\n",
    "        <p class='b' align='center'>text2</p>\n",
    "        <p class='c' align='center'>text3</p>\n",
    "        <div>\n",
    "            <img src='/source' width='300' height='200'>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "# print(contents)\n",
    "for child in contents.children:   #  contents의 자식태그들 가져오기\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec0953-72d5-47b4-8320-a3b236d037b4",
   "metadata": {},
   "source": [
    "## 자신을 포함한 부모 태그까지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8169a7ba-76b4-43b4-af33-da91f608cde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "\n",
      "<div>\n",
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "html = \"\"\"\n",
    "<html>\n",
    "    <head>\n",
    "        <title>crawl</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class='a' align='center'>text1</p>\n",
    "        <p class='b' align='center'>text2</p>\n",
    "        <p class='c' align='center'>text3</p>\n",
    "        <div>\n",
    "            <img src='/source' width='300' height='200'>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "img_tag = contents.find('img')\n",
    "print(img_tag)\n",
    "print()\n",
    "print(img_tag.parent)  # 자신을 포함한 부모 태그까지 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e5d8a-4bb5-4c64-827b-cfdcb8a5cba2",
   "metadata": {},
   "source": [
    "## 특정 부모 태그까지 검색해서 올라가기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "593d7345-e64f-4920-adc1-fad24ee90eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "\n",
      "<body>\n",
      "<p align=\"center\" class=\"a\">text1</p>\n",
      "<p align=\"center\" class=\"b\">text2</p>\n",
      "<p align=\"center\" class=\"c\">text3</p>\n",
      "<div>\n",
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "</div>\n",
      "</body>\n"
     ]
    }
   ],
   "source": [
    "html = \"\"\"\n",
    "<html>\n",
    "    <head>\n",
    "        <title>crawl</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class='a' align='center'>text1</p>\n",
    "        <p class='b' align='center'>text2</p>\n",
    "        <p class='c' align='center'>text3</p>\n",
    "        <div>\n",
    "            <img src='/source' width='300' height='200'>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "img_tag = contents.find('img')\n",
    "print(img_tag)\n",
    "print()\n",
    "print(img_tag.find_parent('body'))  # 역으로 img 태그에서 body태그까지 거슬러 올라감"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe7610-5325-4996-b179-02532b1eeb3c",
   "metadata": {},
   "source": [
    "## 형제 태그 검색\n",
    "- find_next_sibling(): 바로 다음 형제 태그를 검색\n",
    "- find_next_siblings(): 모든 다음 형제 태그를 검색\n",
    "- find_previous_sibling(): 바로 이전 형제 태그를 검색\n",
    "- find_previous_siblings(): 모든 이전 형제 태그를 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66a51519-5111-4737-bc74-42c5b6352618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p align=\"center\" class=\"b\">text2</p>\n",
      "\n",
      "<p align=\"center\" class=\"c\">text3</p>\n",
      "[<p align=\"center\" class=\"c\">text3</p>, <div>\n",
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "</div>]\n"
     ]
    }
   ],
   "source": [
    "html = \"\"\"\n",
    "<html>\n",
    "    <head>\n",
    "        <title>crawl</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class='a' align='center'>text1</p>\n",
    "        <p class='b' align='center'>text2</p>\n",
    "        <p class='c' align='center'>text3</p>\n",
    "        <div>\n",
    "            <img src='/source' width='300' height='200'>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "p_tag = soup.find('p', attrs={'class':'b'})\n",
    "print(p_tag)\n",
    "print()\n",
    "print(p_tag.find_next_sibling())\n",
    "print(p_tag.find_next_siblings())  # 결과를 리스트로 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "501b70fb-8a93-423c-9565-312f2b2a56c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "text1\n",
      "text2\n",
      "text3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "None\n",
      "text1\n"
     ]
    }
   ],
   "source": [
    "html = \"\"\"\n",
    "<html>\n",
    "    <head>\n",
    "        <title>crawl</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class='a' align='center'>text1</p>\n",
    "        <p class='b' align='center'>text2</p>\n",
    "        <p class='c' align='center'>text3</p>\n",
    "        <div>\n",
    "            <img src='/source' width='300' height='200'>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "result = soup.find('body')\n",
    "print(result.text)   # 검색결과의 하부 태그에 있는 모든 텍스트만 검색 결과로 가져옴\n",
    "print(result.string) # 하부 태그가 없는 경우에만 문자열을 가져옴\n",
    "p_tag = result.find('p')\n",
    "print(p_tag.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042be588-5ca0-4b3f-b1f9-3a35b65dbf39",
   "metadata": {},
   "source": [
    "## 검색: find_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9e58438-e80f-421b-a121-71b01ee2f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://www.naver.com')\n",
    "soup = BeautifulSoup(res.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5f46bbf-4047-48a2-9899-4c5d8adb8d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"#topAsideButton\"><span>상단영역 바로가기</span></a>, <a href=\"#shortcutArea\"><span>서비스 메뉴 바로가기</span></a>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all('a', limit=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4121b956-b398-40d8-9fb4-8c9988ae7cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[<span class=\"blind\">검색</span>, <span class=\"blind\">입력도구</span>, <span class=\"blind\">자동완성/최근검색어펼치기</span>]\n"
     ]
    }
   ],
   "source": [
    "result = soup.find_all('span', attrs={'class':'blind'})\n",
    "print(len(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2976338-ccf2-42a3-9459-3c0eda0527a1",
   "metadata": {},
   "source": [
    "[실습] 네이버 뉴스 페이지에 언론사 목록 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ecba1c8-1067-4bc1-8234-402e9944a5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아이뉴스24', '블로터', '대전일보', '서울경제', '대구MBC', '미디어오늘', '데일리안', '디지털데일리', '프레시안', '디지털타임스']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "\n",
    "res = requests.get('https://news.naver.com')\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "result = soup.find_all('h4', attrs={'class':\"channel\"})\n",
    "#print(result[0])\n",
    "#print(list(result[0].children)[0])\n",
    "\n",
    "news_list = [list(tag.children)[0] for tag in result]\n",
    "print(news_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7306dfc8-9272-4583-b28e-de059d68cca6",
   "metadata": {},
   "source": [
    "## 검색: select_one(), select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b55c32fe-d192-4953-a0ad-c9fb4911a52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h3 class=\"tit\">공지사항</h3>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('http://tradecampus.com')\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "print(soup.select_one('body > div > div.wrapper.main_page > div.renew_main > div.col-12 > div > div.renew_main_notice > div > div > h3'))   # copy select, 원하는 태그 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03ca4ba9-fc1e-4364-b6e3-e7530cc916ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "오프라인/Live\n",
      "\n",
      "제1기 전자상거래(B2C)수출실무 첫걸음 개강안내(10/27)\n",
      "2023-10-24\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tradecampus.com 메인 페이지에 공지사항 2번째 내용 가져오기\n",
    "\n",
    "notice = soup.select('body > div > div.wrapper.main_page > div.renew_main > div.col-12 > div > div.renew_main_notice > div > ul > li:nth-child(2)')   # 2 번째 자식요소에 해당하는 li 요소\n",
    "#print(notice)\n",
    "print(notice[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de0a3157-e338-4082-a533-e072ca3ab55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "\n",
      "e러닝\n",
      "\n",
      "[GTEP] '24년~'26년 신규 사업단(대학) 모집 안내 (~11/24)\n",
      "2023-10-27\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "오프라인/Live\n",
      "\n",
      "제1기 전자상거래(B2C)수출실무 첫걸음 개강안내(10/27)\n",
      "2023-10-24\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "자격시험\n",
      "\n",
      "(11월 25일 시행) 제56회 국제무역사 1급 자격시험 접수안내(10/30(월)~11/12(일), 예스24티켓에서 신청)\n",
      "2023-10-23\n",
      "\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# :nth-child(n) 삭제시 모슨 요소 가져옴\n",
    "notice = soup.select('body > div > div.wrapper.main_page > div.renew_main > div.col-12 > div > div.renew_main_notice > div > ul > li')\n",
    "print(len(notice))\n",
    "for n in notice:\n",
    "    print(n.text)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05af6ae9-eaa2-4461-ac8a-812de89c5bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "고객센터\n",
      "\n",
      "\n",
      "오프라인 교육\n",
      "02-6000-5378/5379\n",
      "\n",
      "\n",
      "e러닝\n",
      "02-6000-6251\n",
      "\n",
      "\n",
      "운영시간\n",
      "평일 09:00~18:00 (주말/공휴일 : 휴무)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 고객센터 영역 텍스트 가져오기\n",
    "text = soup.find('div', attrs={'class':'serviceInfo'})\n",
    "#print(text)\n",
    "print(text.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1bc7564-18d6-4ebf-ba8c-11b55b8c927a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/weven_template_repository/theme/KITAAC/1/resource/img/img_main_cate01.png\n"
     ]
    }
   ],
   "source": [
    "ele = soup.find('span', attrs={'class':'img'})\n",
    "#print(ele)\n",
    "ele = ele.find('img').get('src')\n",
    "print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf083b9-8b92-41fc-9d1a-856808edea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d42c7d-4aea-48da-bf1e-ad411301c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1155fc-2fd5-464d-b0d8-8668e6278363",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012538e7-fe80-465a-ad50-1c4d4b84bcb9",
   "metadata": {},
   "source": [
    "[실습] 네이버 웹툰 제목 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f4868d-9bf6-420c-bdb1-d0fa10ad4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# 크롬객체 생성 [서비스 속성 = (서비스객체 생성-> 크롬 드라이버매니저 설치), 옵션 속성 = 크롬옵션 객체 생성]\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()),options=ChromeOptions()) \n",
    "url='https://comic.naver.com/webtoon'\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(5)  # 동적으로 생성되는 페이지의 내용이 완성될 때까지 대기\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# 요일별 전체 웹툰의 css selector임\n",
    "temp = soup.select_one('#container > div.component_wrap.type2 > div.WeekdayMainView__daily_all_wrap--UvRFc')\n",
    "#print(temp) # 홈페이지가 동적으로 작동함;; 크롤링 방지를 위해 막아놓음, BS 만으로 크롤링 불가\n",
    "\n",
    "# 요일별 div 태그 (find_all() 함수로 동일 클래스 선택자를 사용하는 모든 태그 검색)\n",
    "temp = temp.find_all('div',attrs={'class':'WeekdayMainView__daily_all_item--DnTAH'})\n",
    "#print(len(temp)) #7\n",
    "\n",
    "week=['월','화','수','목','금','토','일']\n",
    "\n",
    "for i,w in enumerate(temp):\n",
    "    print(f'========== {week[i]}요일 웹툰 ========== ')\n",
    "    week_list = w.select('ul>li') #요일 웹툰 li리스트 : 상위태그가 ul인 모든 li 리스트 가져옴\n",
    "    for li_tag in week_list:\n",
    "        print(li_tag.find('span',attrs={'class':'text'}).text) #.string도 가능, li태그의 제목 위치 찾아서 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d484b02d-8624-461f-ab22-29c97b5e245c",
   "metadata": {},
   "source": [
    "[실습] 다음 영화 사이트에서 영화 포스터 다운로드 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72c37d4d-67dd-4260-bcac-df2461ec020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 테스트\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://movie.daum.net/ranking/reservation')\n",
    "res.raise_for_status()  # 응답없을 경우 예외 발생 후 중지시킴\n",
    "\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "poster_img = soup.find_all('img', attrs={'class':'img_thumb'})\n",
    "# print(len(poster_img))\n",
    "img_src = poster_img[0].get('src')\n",
    "res = requests.get(img_src)        # src  속성을 통해 얻은 url을 가져오기\n",
    "with open('poster.jpg', 'wb') as f: \n",
    "    f.write(res.content)   # content: 텍스트가 아닌 바이너리 데이터 가져오기, 가져온 url을 이미지로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1cb7269-c898-4a82-bd82-6adae3eca97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더가 존재함\n",
      "1 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2F814364da4037f74217243eeee6b4648d7dc2fc79\n",
      "2 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2Fd4c29f7ca7d107dd1c84841e999aba7952e1922d\n",
      "3 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2F3024cd158a242c789367a861db53c9f4470a14d9\n",
      "4 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2Fc96eeca7ae58927348a05c4104ba3fc64ff1b03d\n",
      "5 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2Fa92b521216c7cf6d9d714d798d47bfbb46e5d806\n",
      "6 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2Fb1c637d374c7f62cde230dcaa8121822f76a3c75\n",
      "7 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2Fe6e4aa9fa401a9b50a740e2d663cd3680f404cd8\n",
      "8 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2F8a87a8c7a462e24cb77b63b10d1497966f7c6210\n",
      "9 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2F74b598df4a67af2add11f2bb6cc62b12de45a0ac\n",
      "10 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2F8a41fde7f5dda3d369818fa80baf3b2472c789b7\n",
      "11 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2F4196e4cdc819ef9c288b040035a366cb0729fbad\n",
      "12 : https://img1.daumcdn.net/thumb/C408x596/?fname=http%3A%2F%2Ft1.daumcdn.net%2Fmovie%2F0357a82b7226464b87072c0b8d2246b71567986846719\n",
      "13 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2F98309d0adcd5c39ac18da8d4aba211a100110517\n",
      "14 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2F8bdd6c13267c4334f270b7752941ae185d8d9ac7\n",
      "15 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2F0965d6d350f199711ffcc30e590f99fc6181dab8\n",
      "16 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2F54d73561dce387c9a482cee6a47beacb6318d18e\n",
      "17 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2F1b4da55a56da0929a424f844307acbeda8bad4a0\n",
      "18 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2F73721613b87046de026b07e14996847f932f6f84\n",
      "19 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2F6505ab8e4828b7fa98d019f9c4de58f8cbc87204\n",
      "20 : https://img1.daumcdn.net/thumb/C408x596/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fmovie%2F0be484640c05656bbba44ce40ff47595b59141fe\n",
      "포스터 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://movie.daum.net/ranking/reservation')\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "poster_img = soup.find_all('img', attrs={'class':'img_thumb'})   # img 태그에 있는 class속성의 img_thumb값을 가진 태그 값 가져오기\n",
    "\n",
    "# 이미지를 저장할 폴더 생성(폴더가 있으면 안만들고, 없으면 만든다)\n",
    "img_dir = './poster_img/'\n",
    "\n",
    "if not os.path.exists(img_dir):    # 폴더가 존재하는지 여부\n",
    "    os.makedirs(img_dir)      # 폴더 생성\n",
    "    print('폴더 생성 완료')\n",
    "else:\n",
    "    print('폴더가 존재함')\n",
    "\n",
    "for i, movie in enumerate(poster_img, 1):   # 인덱스가 1부터 시작하도록 지정\n",
    "    title = movie.get('alt')     # 영화제목; alt속성에 있음\n",
    "    img_url = movie.get('src')   # 이미지의 url; src속성에 있음 =\n",
    "\n",
    "    print(i, ':', img_url)\n",
    "    img_res = requests.get(img_url)  # 이미지의 url 가져오기\n",
    "\n",
    "    if ':' in title:              # 영화 제목의 ':' 처리\n",
    "        title  = title.replace(':', ' ')\n",
    "        \n",
    "    with open(img_dir+f'm{i:02d}_{title}.jpg', 'wb') as f:\n",
    "        f.write(img_res.content)\n",
    "\n",
    "print('포스터 저장 완료')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d474a1a6-696a-4bc9-a17b-1bb8652ce564",
   "metadata": {},
   "source": [
    "[실습] 네이버 뉴스에서 경제 관련 언론사별 랭킹뉴스 추출하기\n",
    "<pre>\n",
    "    - 네이버 뉴스 랭킹 페이지에서 언론사의 이름에 '경제'단어가 포함된 언론사의 기사 제목을 추출\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "705dc2df-df02-47f4-a6e2-4a92df2f6b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "등록 언론사 개수:  82\n",
      "언론사: 서울경제\n",
      "1: “별풍선 24억이면 결혼한다”더니…아프리카TV 인기 어마어마하네\n",
      "2: \"전청조에게 가스라이팅 당했다? 남현희 말 백 번 의심스러워” 이수정 주장\n",
      "3: 여고생 집까지 가서 술 먹이고 성폭행한 30대…기간제 교사였다\n",
      "4: '전청조 패러디' 또 등장…“전충주, I am 충주예요” 어떻게 보십니까\n",
      "5: “오빠, 나 10대인데 모텔서 보자”더니…'참교육' 나온 유튜버였다\n",
      "--------------------------------------------------\n",
      "언론사: 한국경제\n",
      "1: \"땅만 보고 다녀야겠어요\"…여의도 직장인 덮친 '공포' [돈앤톡]\n",
      "2: \"삼성 때문에 다 망한다\" 맹렬한 반대…54년 만에 '대반전' [김익환의 컴퍼니워치]\n",
      "3: [단독] 연예인·오너들도 당했다…1000억 폰지사기범 구속\n",
      "4: \"왕복 20만원이면 차라리 해외 가죠\"…제주 골프장 '위기'\n",
      "5: \"무조건 오를 줄 알았는데…\" 수백억 베팅한 개미들 '비명'\n",
      "--------------------------------------------------\n",
      "언론사: 매일경제\n",
      "1: “한국서 살고 싶어요”…현금 10억씩 들고 몰려오는 중국인들\n",
      "2: “아이 보는데 남편 폭행당했다”…가해자 아들은 ‘네 아빠 발렸다’ 조롱\n",
      "3: “도와달라”는 인요한에... 이준석 “그렇게 하셔봤자 아무도 신경 안써”\n",
      "4: “비트코인, 2년 내 2억 간다”…4배 이상 오른다고 전망한 美 번스타인\n",
      "5: “오은영이 멘탈 코치” 전청조, 1인당 3억 제안했다\n",
      "--------------------------------------------------\n",
      "언론사: 아시아경제\n",
      "1: \"전청조, 샤넬 매장서 남현희를 공주 대접\" 목격담 화제\n",
      "2: \"도망가면 어쩌려고\" 韓부동산 산 중국인, 4명 중 1명은 국내은행서 돈 빌려\n",
      "3: '오늘 아침' 리포터 김태민, 뇌출혈로 45세 사망\n",
      "4: \"알바 월급도 부담\"…나홀로 사장님 437만명, 15년 만에 최대\n",
      "5: 5대 은행 연봉 '1억원 훌쩍'…퇴직금은 '최대 6억원'(종합)\n",
      "--------------------------------------------------\n",
      "언론사: 한국경제TV\n",
      "1: \"비트코인, 2025년까지 5배 폭등\"…대형 이벤트 임박\n",
      "2: 돼지심장 이식 50대 환자 또 사망\n",
      "3: 등·뒷목에도 잘 생기는 켈로이드…조기 발견 중요\n",
      "4: 야금야금 사들였다…3월 이후 최고 상승률\n",
      "5: 이자장사로 떼돈 벌더니…'억' 소리 나는 은행원 연봉\n",
      "--------------------------------------------------\n",
      "언론사: 헤럴드경제\n",
      "1: “카카오에 억단위 투자했습니다. 자다가도 눈이 떠지네요” [투자360]\n",
      "2: \"OO 배우 닮았어요\"…전청조가 남자 유혹할 때 쓴 사진\n",
      "3: “BJ에게 3억7천만원 뿌린 시청자, 실화냐?” 아프리카TV 누가 보나 했더니 이 정도?\n",
      "4: “엄마, 사탕맛 이상해” 핼러윈에 ‘마약간식’ 받은 어린이들…美발칵\n",
      "5: '이혼' 최동석, 박지윤 귀책 루머에…\"조작·왜곡, 억측 강경대응할 것\"\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36'}\n",
    "\n",
    "res = requests.get('https://news.naver.com/main/ranking/popularDay.naver', headers=headers)\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "# 전체 페이지에서 한 언론사에 해당하는 가장 큰 태그\n",
    "my_news = soup.find_all('div', attrs={'class':'rankingnews_box'})\n",
    "print('등록 언론사 개수: ', len(news_name))  \n",
    "\n",
    "# 언론사 별로 순회\n",
    "for n in my_news:\n",
    "    press_name = n.find('strong').text  # 언론사 이름 찾기, 언론사별로는 하나니까 find()\n",
    "    if '경제' in press_name:   # '경제' 가 들어가 있는지 여부\n",
    "        print('언론사:',press_name)\n",
    "        news_title = n.find_all('div', attrs={'class':'list_content'}) # 기사 제목을 포함한 태그가 들어가있는 태그\n",
    "        for i, rankingnews in enumerate(news_title, 1):\n",
    "            news_ranking_title = rankingnews.find('a').text   # 기사 제목 포함한 태그\n",
    "            print(f'{i}: {news_ranking_title}')\n",
    "        print('-'*50)\n",
    "\n",
    "# 크롤링하는 웹페이지의 태그가 어떻게 구성되어있는지, 상위 하위는 어떤 것인지 파악하여 웹 크롤링 필요 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe175e81-bc00-4a41-a831-6a45bed69444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
